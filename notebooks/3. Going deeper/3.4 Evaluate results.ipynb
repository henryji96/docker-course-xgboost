{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:100%\" src=\"../images/practical_xgboost_in_python_notebook_header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results\n",
    "In this notebook you will see how measure the quality of the algorithm performance.\n",
    "\n",
    "**You will learn how to:**\n",
    "- <a href=\"#pmetrics\">use predefined evaluation metrics</a>,\n",
    "- <a href=\"#cmetrics\">write your own evaluation metrics</a>,\n",
    "- <a href=\"#earlystopping\">use early stopping feature</a>,\n",
    "- <a href=\"#cv\">cross validate results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Begin with loading all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load agaricus dataset from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Agaricus data\n",
    "dtrain = xgb.DMatrix('../data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('../data/agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify training parameters - we are going to use 5 decision tree stumps with average learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify general training parameters\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':1,\n",
    "    'silent':1,\n",
    "    'eta':0.5\n",
    "}\n",
    "\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model let's also specify `watchlist` array to observe it's performance on the both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watchlist  = [(dtest,'test'), (dtrain,'train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using predefined evaluation metrics<a name='pmetrics' />\n",
    "\n",
    "#### What is already available?\n",
    "There are already [some](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md) predefined metrics availabe. You can use them as the input for the `eval_metric` parameter while training the model.\n",
    "\n",
    "- `rmse` - [root mean square error](https://www.wikiwand.com/en/Root-mean-square_deviation),\n",
    "- `mae` - [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error?oldformat=true),\n",
    "- `logloss` - [negative log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function?oldformat=true)\n",
    "- `error` - binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. Treat predicted values with probability $p > 0.5$ as positive,\n",
    "- `merror` - multiclass classification error rate. It is calculated as `#(wrong cases)/#(all cases)`,\n",
    "- `auc` - [area under curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic?oldformat=true),\n",
    "- `ndcg` - [normalized discounted cumulative gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain?oldformat=true),\n",
    "- `map` - [mean average precision](https://en.wikipedia.org/wiki/Information_retrieval?oldformat=true)\n",
    "\n",
    "By default an `error` metric will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "[1]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "[2]\ttest-error:0.03352\ttrain-error:0.030401\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.021495\n",
      "[4]\ttest-error:0.031037\ttrain-error:0.025487\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change is simply specify the `eval_metric` argument to the `params` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttrain-logloss:0.460108\n",
      "[1]\ttest-logloss:0.383911\ttrain-logloss:0.378728\n",
      "[2]\ttest-logloss:0.312678\ttrain-logloss:0.308061\n",
      "[3]\ttest-logloss:0.26912\ttrain-logloss:0.26139\n",
      "[4]\ttest-logloss:0.239746\ttrain-logloss:0.232174\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use multiple evaluation metrics at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttest-auc:0.892138\ttrain-logloss:0.460108\ttrain-auc:0.888997\n",
      "[1]\ttest-logloss:0.383911\ttest-auc:0.938901\ttrain-logloss:0.378728\ttrain-auc:0.942881\n",
      "[2]\ttest-logloss:0.312678\ttest-auc:0.976157\ttrain-logloss:0.308061\ttrain-auc:0.981415\n",
      "[3]\ttest-logloss:0.26912\ttest-auc:0.979685\ttrain-logloss:0.26139\ttrain-auc:0.985158\n",
      "[4]\ttest-logloss:0.239746\ttest-auc:0.9785\ttrain-logloss:0.232174\ttrain-auc:0.983744\n"
     ]
    }
   ],
   "source": [
    "params['eval_metric'] = ['logloss', 'auc']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom evaluation metric<a name='cmetrics' />\n",
    "\n",
    "In order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - predicted probabilities and `DMatrix` object holding training data.\n",
    "\n",
    "In this example our classification metric will simply count the number of misclassified examples assuming that classes with $p> 0.5$ are positive. You can change this threshold if you want more certainty. \n",
    "\n",
    "The algorithm is getting better when the number of misclassified examples is getting lower. Remember to also set the argument `maximize=False` while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom evaluation metric\n",
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label() # obtain true labels\n",
    "    preds = pred_probs > 0.5 # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttest-auc:0.892138\ttrain-logloss:0.460108\ttrain-auc:0.888997\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[1]\ttest-logloss:0.383911\ttest-auc:0.938901\ttrain-logloss:0.378728\ttrain-auc:0.942881\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[2]\ttest-logloss:0.312678\ttest-auc:0.976157\ttrain-logloss:0.308061\ttrain-auc:0.981415\ttest-misclassified:54\ttrain-misclassified:198\n",
      "[3]\ttest-logloss:0.26912\ttest-auc:0.979685\ttrain-logloss:0.26139\ttrain-auc:0.985158\ttest-misclassified:44\ttrain-misclassified:140\n",
      "[4]\ttest-logloss:0.239746\ttest-auc:0.9785\ttrain-logloss:0.232174\ttrain-auc:0.983744\ttest-misclassified:50\ttrain-misclassified:166\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, feval=misclassified, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though the `params` dictionary is holding `eval_metric` key these values are being ignored and overwritten by `feval`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the evaluation results\n",
    "You can get evaluation scores by declaring a dictionary for holding values and passing it as a parameter for `evals_result` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.457887\ttest-auc:0.892138\ttrain-logloss:0.460108\ttrain-auc:0.888997\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[1]\ttest-logloss:0.383911\ttest-auc:0.938901\ttrain-logloss:0.378728\ttrain-auc:0.942881\ttest-misclassified:178\ttrain-misclassified:742\n",
      "[2]\ttest-logloss:0.312678\ttest-auc:0.976157\ttrain-logloss:0.308061\ttrain-auc:0.981415\ttest-misclassified:54\ttrain-misclassified:198\n",
      "[3]\ttest-logloss:0.26912\ttest-auc:0.979685\ttrain-logloss:0.26139\ttrain-auc:0.985158\ttest-misclassified:44\ttrain-misclassified:140\n",
      "[4]\ttest-logloss:0.239746\ttest-auc:0.9785\ttrain-logloss:0.232174\ttrain-auc:0.983744\ttest-misclassified:50\ttrain-misclassified:166\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, feval=misclassified, maximize=False, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can reuse these scores (i.e. for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': {'auc': [0.892138, 0.938901, 0.976157, 0.979685, 0.9785],\n",
      "          'logloss': [0.457887, 0.383911, 0.312678, 0.26912, 0.239746],\n",
      "          'misclassified': [178.0, 178.0, 54.0, 44.0, 50.0]},\n",
      " 'train': {'auc': [0.888997, 0.942881, 0.981415, 0.985158, 0.983744],\n",
      "           'logloss': [0.460108, 0.378728, 0.308061, 0.26139, 0.232174],\n",
      "           'misclassified': [742.0, 742.0, 198.0, 140.0, 166.0]}}\n"
     ]
    }
   ],
   "source": [
    "pprint(evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping<a name='earlystopping' />\n",
    "There is a nice optimization trick when fitting multiple trees. \n",
    "\n",
    "You can train the model until the validation score **stops** improving. Validation error needs to decrease at least every `early_stopping_rounds` to continue training. This approach results in simpler model, because the lowest number of trees will be found (simplicity).\n",
    "\n",
    "In the following example a total number of 1500 trees is to be created, but we are telling it to stop if the validation score does not improve for last ten iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "Multiple eval metrics have been passed: 'train-error' will be used for early stopping.\n",
      "\n",
      "Will train until train-error hasn't improved in 10 rounds.\n",
      "[1]\ttest-error:0.11049\ttrain-error:0.113926\n",
      "[2]\ttest-error:0.03352\ttrain-error:0.030401\n",
      "[3]\ttest-error:0.027312\ttrain-error:0.021495\n",
      "[4]\ttest-error:0.031037\ttrain-error:0.025487\n",
      "[5]\ttest-error:0.019243\ttrain-error:0.01735\n",
      "[6]\ttest-error:0.019243\ttrain-error:0.01735\n",
      "[7]\ttest-error:0.015518\ttrain-error:0.013358\n",
      "[8]\ttest-error:0.015518\ttrain-error:0.013358\n",
      "[9]\ttest-error:0.009311\ttrain-error:0.007523\n",
      "[10]\ttest-error:0.015518\ttrain-error:0.013358\n",
      "[11]\ttest-error:0.019243\ttrain-error:0.01735\n",
      "[12]\ttest-error:0.009311\ttrain-error:0.007523\n",
      "[13]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[14]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[15]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[16]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[17]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[18]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[19]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[20]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[21]\ttest-error:0.005587\ttrain-error:0.005988\n",
      "[22]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "[23]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "Stopping. Best iteration:\n",
      "[13]\ttest-error:0.001862\ttrain-error:0.001996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#params['eval_metric'] = 'error'\n",
    "params['eval_metric'] = ['error']\n",
    "num_rounds = 1500\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `early_stopping_rounds` parameter resulting model will have 3 additional fields - `bst.best_score`, `bst.best_iteration` and `bst.best_ntree_limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booster best train score: 0.001996\n",
      "Booster best iteration: 13\n",
      "Booster best number of trees limit: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Booster best train score: {}\".format(bst.best_score))\n",
    "print(\"Booster best iteration: {}\".format(bst.best_iteration))\n",
    "print(\"Booster best number of trees limit: {}\".format(bst.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also keep in mind that `train()` will return a model from the last iteration, not the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating results<a name='cv' />\n",
    "Native package provides an option for cross-validating results (but not as sophisticated as Sklearn package). The next input shows a basic execution. Notice that we are passing only single `DMatrix`, so it would be good to merge train and test into one object to have more training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113927</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.113926</td>\n",
       "      <td>0.001080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113927</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.113926</td>\n",
       "      <td>0.001080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030402</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025488</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.025487</td>\n",
       "      <td>0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.004099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.017350</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.017350</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014433</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011822</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0         0.113927        0.009725          0.113926         0.001080\n",
       "1         0.113927        0.009725          0.113926         0.001080\n",
       "2         0.030402        0.007197          0.030401         0.000800\n",
       "3         0.021496        0.004343          0.021496         0.000483\n",
       "4         0.025488        0.005672          0.025487         0.000630\n",
       "5         0.021957        0.006446          0.020455         0.004099\n",
       "6         0.017350        0.005540          0.017350         0.000615\n",
       "7         0.014433        0.004964          0.014126         0.001435\n",
       "8         0.013358        0.003075          0.013358         0.000342\n",
       "9         0.011822        0.004345          0.011584         0.002612"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_rounds = 10 # how many estimators\n",
    "hist = xgb.cv(params, dtrain, num_rounds, nfold=10,stratified=True, metrics={'error'}, seed=seed)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "- by default we get a pandas data frame object (can be changed with `as_pandas` param),\n",
    "- metrics are passed as an argument (muliple values are allowed),\n",
    "- we can use own evaluation metrics (param `feval` and `maximize`),\n",
    "- we can use early stopping feature (param `early_stopping_rounds`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
